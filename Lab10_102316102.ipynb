{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31eab02",
   "metadata": {},
   "source": [
    "**Assignment 10 CC By Prithvii Ahuja(102316102)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0186f1",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuaƟon using re.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Split using split() and word_tokenize() and compare how Python split and NLTK’s\n",
    "word_tokenize() differ.\n",
    "4. Remove stopwords (using NLTK's stopwords list).\n",
    "5. Display word frequency distribuƟon (excluding stopwords).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306f8edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: \n",
      "cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence \n",
      "machine learning and natural language processing these systems learn from data recognize patterns and assist in \n",
      "decisionmaking they aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information\n",
      "\n",
      "Split(): ['cognitive', 'computing', 'refers', 'to', 'advanced', 'systems', 'that', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'these', 'systems', 'learn', 'from', 'data', 'recognize', 'patterns', 'and', 'assist', 'in', 'decisionmaking', 'they', 'aim', 'to', 'augment', 'human', 'intelligence', 'by', 'mimicking', 'the', 'way', 'the', 'human', 'brain', 'analyzes', 'and', 'responds', 'to', 'information']\n",
      "word_tokenize(): ['cognitive', 'computing', 'refers', 'to', 'advanced', 'systems', 'that', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'these', 'systems', 'learn', 'from', 'data', 'recognize', 'patterns', 'and', 'assist', 'in', 'decisionmaking', 'they', 'aim', 'to', 'augment', 'human', 'intelligence', 'by', 'mimicking', 'the', 'way', 'the', 'human', 'brain', 'analyzes', 'and', 'responds', 'to', 'information']\n",
      "Filtered Words: ['cognitive', 'computing', 'refers', 'advanced', 'systems', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'systems', 'learn', 'data', 'recognize', 'patterns', 'assist', 'decisionmaking', 'aim', 'augment', 'human', 'intelligence', 'mimicking', 'way', 'human', 'brain', 'analyzes', 'responds', 'information']\n",
      "Word Frequency: Counter({'human': 3, 'systems': 2, 'intelligence': 2, 'cognitive': 1, 'computing': 1, 'refers': 1, 'advanced': 1, 'simulate': 1, 'thought': 1, 'processes': 1, 'using': 1, 'artificial': 1, 'machine': 1, 'learning': 1, 'natural': 1, 'language': 1, 'processing': 1, 'learn': 1, 'data': 1, 'recognize': 1, 'patterns': 1, 'assist': 1, 'decisionmaking': 1, 'aim': 1, 'augment': 1, 'mimicking': 1, 'way': 1, 'brain': 1, 'analyzes': 1, 'responds': 1, 'information': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Prithvi\n",
      "[nltk_data]     Ahuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "paragraph=\"\"\"\n",
    "Cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence, \n",
    "machine learning, and natural language processing. These systems learn from data, recognize patterns, and assist in \n",
    "decision-making. They aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Lowercase and remove punctuation\n",
    "text_clean = re.sub(r'[^\\w\\s]', '', paragraph.lower())\n",
    "print(\"Cleaned Text:\", text_clean)\n",
    "\n",
    "# 2. Tokenize sentences and words\n",
    "sent_tokens = sent_tokenize(paragraph)\n",
    "word_tokens_nltk = word_tokenize(text_clean)\n",
    "word_tokens_split = text_clean.split()\n",
    "\n",
    "# 3. Compare split() vs word_tokenize()\n",
    "print(\"Split():\", word_tokens_split)\n",
    "print(\"word_tokenize():\", word_tokens_nltk)\n",
    "\n",
    "# 4. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens_nltk if word not in stop_words]\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "\n",
    "# 5. Word Frequency Distribution\n",
    "freq_dist = Counter(filtered_words)\n",
    "print(\"Word Frequency:\", freq_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d593d9",
   "metadata": {},
   "source": [
    "Q2. Using the same paragraph from Q1:\n",
    "1. Extract all words with only alphabets using re.findall()\n",
    "2. Remove stop words using NLTK’s stopword list\n",
    "3. Perform stemming with PorterStemmer\n",
    "4. Perform lemmaƟzaƟon with WordNetLemmaƟzer\n",
    "5. Compare the stemmed and lemmaƟzed outputs and explain when you’d prefer one over\n",
    "the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bbac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prithvi\n",
      "[nltk_data]     Ahuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: ['cognit', 'comput', 'refer', 'advanc', 'system', 'simul', 'human', 'thought', 'process', 'use', 'artifici', 'intellig', 'machin', 'learn', 'natur', 'languag', 'process', 'system', 'learn', 'data', 'recogn', 'pattern', 'assist', 'decisionmak', 'aim', 'augment', 'human', 'intellig', 'mimick', 'way', 'human', 'brain', 'analyz', 'respond', 'inform']\n",
      "Lemmatized: ['cognitive', 'computing', 'refers', 'advanced', 'system', 'simulate', 'human', 'thought', 'process', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'system', 'learn', 'data', 'recognize', 'pattern', 'assist', 'decisionmaking', 'aim', 'augment', 'human', 'intelligence', 'mimicking', 'way', 'human', 'brain', 'analyzes', 'responds', 'information']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1. Extract alphabet-only words\n",
    "words_alpha = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
    "\n",
    "# 2. Remove stopwords again\n",
    "filtered_alpha = [word for word in words_alpha if word not in stop_words]\n",
    "\n",
    "# 3. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_alpha]\n",
    "\n",
    "# 4. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_alpha]\n",
    "\n",
    "# 5. Comparison\n",
    "print(\"Stemmed:\", stemmed_words)\n",
    "print(\"Lemmatized:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eef14f",
   "metadata": {},
   "source": [
    "Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n",
    "1. Use CountVectorizer to generate the Bag of Words representaƟon.\n",
    "2. Use TfidfVectorizer to compute TF-IDF scores.\n",
    "3. Print and interpret the top 3 keywords from each text using TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f333dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Features:\n",
      " ['ahead' 'amarnath' 'attack' 'concerns' 'deadly' 'despite' 'enhanced' 'in'\n",
      " 'indo' 'measures' 'of' 'pahalgam' 'pak' 'security' 'sees' 'sparks'\n",
      " 'surge' 'tensions' 'tourism' 'yatra']\n",
      "Text 1 Top Keywords: ['tensions', 'sparks', 'attack']\n",
      "Text 2 Top Keywords: ['yatra', 'of', 'amarnath']\n",
      "Text 3 Top Keywords: ['surge', 'sees', 'concerns']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "texts = [\n",
    "    \"Deadly Attack in Pahalgam Sparks Indo-Pak Tensions\",\n",
    "    \"Enhanced Security Measures in Pahalgam Ahead of Amarnath Yatra\",\n",
    "    \"Pahalgam Sees Surge in Tourism Despite Security Concerns.\"\n",
    "]\n",
    "\n",
    "# 1. Bag of Words\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(texts)\n",
    "print(\"BoW Features:\\n\", cv.get_feature_names_out())\n",
    "\n",
    "# 2. TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "# 3. Top 3 keywords\n",
    "import numpy as np\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    scores = tfidf_matrix[i].toarray().flatten()\n",
    "    top_indices = np.argsort(scores)[-3:][::-1]\n",
    "    top_words = [tfidf.get_feature_names_out()[j] for j in top_indices]\n",
    "    print(f\"Text {i+1} Top Keywords: {top_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b00a8",
   "metadata": {},
   "source": [
    "Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs\n",
    "Blockchain).\n",
    "1. Preprocess and tokenize both texts.\n",
    "2. Calculate:\n",
    "a. Jaccard Similarity using sets\n",
    "b. Cosine Similarity using TfidfVectorizer + cosine_similarity()\n",
    "c. Analyze which similarity metric gives beƩer insights in your case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc2f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.08955223880597014\n",
      "Cosine Similarity: 0.15266297735074663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "text1 = \"Artificial Intelligence enables machines to mimic human intelligence. It powers applications like voice assistants, image recognition, and recommendation systems. AI learns from data, improving its performance over time. It’s widely used in healthcare, finance, and transportation to automate tasks and enhance decision-making.\"\n",
    "text2 = \"Blockchain is a decentralized digital ledger that records transactions securely and transparently. Each block in the chain contains data that is immutable once verified. It underpins cryptocurrencies like Bitcoin and supports smart contracts. Blockchain is revolutionizing finance, supply chains, and digital identity management.\"\n",
    "\n",
    "# 1. Preprocess\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return set(word_tokenize(text))\n",
    "\n",
    "set1, set2 = preprocess(text1), preprocess(text2)\n",
    "\n",
    "# 2a. Jaccard Similarity\n",
    "jaccard = len(set1 & set2) / len(set1 | set2)\n",
    "print(\"Jaccard Similarity:\", jaccard)\n",
    "\n",
    "# 2b. Cosine Similarity\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf.fit_transform([text1, text2])\n",
    "cosine = cosine_similarity(tfidf_vectors[0], tfidf_vectors[1])[0][0]\n",
    "print(\"Cosine Similarity:\", cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67d91d",
   "metadata": {},
   "source": [
    "Q5. Write a short review for a product or service.\n",
    "1. Use TextBlob or VADER to find polarity & subjecƟvity for each review.\n",
    "2. Classify reviews into PosiƟve / NegaƟve / Neutral.\n",
    "3. Create a word cloud using the wordcloud library for all posiƟve reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beff3998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: -0.9099999999999998\n",
      "Subjectivity: 0.8666666666666667\n",
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review = \"The trimmers quality is very bad its machine broke within a week\"\n",
    "\n",
    "# 1. Sentiment\n",
    "blob = TextBlob(review)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n",
    "\n",
    "# 2. Classification\n",
    "if blob.sentiment.polarity > 0.1:\n",
    "    sentiment = \"Positive\"\n",
    "elif blob.sentiment.polarity < -0.1:\n",
    "    sentiment = \"Negative\"\n",
    "else:\n",
    "    sentiment = \"Neutral\"\n",
    "print(\"Sentiment:\", sentiment)\n",
    "\n",
    "# 3. WordCloud (for positive reviews)\n",
    "if sentiment == \"Positive\":\n",
    "    wc = WordCloud(width=500, height=300, background_color='white').generate(review)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Word Cloud - Positive Review\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad508d9",
   "metadata": {},
   "source": [
    "Q6. Choose your own paragraph (~100 words) as training data.\n",
    "1. Tokenize text using Tokenizer() from keras.preprocessing.text\n",
    "2. Create input sequences and build a simple LSTM or Dense model\n",
    "3. Train the model and generate 2–3 new lines of text starƟng from any seed word you\n",
    "provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6f834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: cognitive refers computing refers computing simulate\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "text = \"\"\"Cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence, \n",
    "machine learning, and natural language processing. These systems learn from data, recognize patterns, and assist in \n",
    "decision-making. They aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information.\"\"\"\n",
    "\n",
    "# 1. Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 2. Creating n-gram sequences\n",
    "input_sequences = []\n",
    "words = text.lower().split()\n",
    "for i in range(1, len(words)):\n",
    "    n_gram_sequence = words[:i+1]\n",
    "    sequence = tokenizer.texts_to_sequences([' '.join(n_gram_sequence)])[0]\n",
    "    input_sequences.append(sequence)\n",
    "\n",
    "# Padding sequence\n",
    "max_len = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# 3. Model definition\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=total_words, output_dim=10, input_length=max_len - 1),\n",
    "    LSTM(50),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# 4. Generate new text\n",
    "seed = \"cognitive\"\n",
    "for _ in range(5):\n",
    "    token_seq = tokenizer.texts_to_sequences([seed])[0]\n",
    "    token_seq = pad_sequences([token_seq], maxlen=max_len - 1, padding='pre')\n",
    "    predicted_probs = model.predict(token_seq, verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    next_word = tokenizer.index_word.get(predicted_index, '')\n",
    "    seed += \" \" + next_word\n",
    "\n",
    "print(\"Generated Text:\", seed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
