{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 9 CC by Prithvii Ahuja(102316102)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuaƟon.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribuƟon (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Prithvi\n",
      "[nltk_data]     Ahuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Prithvi\n",
      "[nltk_data]     Ahuja\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "\n",
      "cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence \n",
      "machine learning and natural language processing these systems learn from data recognize patterns and assist in \n",
      "decisionmaking they aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information\n",
      "\n",
      "\n",
      "Tokens:\n",
      "['cognitive', 'computing', 'refers', 'to', 'advanced', 'systems', 'that', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'these', 'systems', 'learn', 'from', 'data', 'recognize', 'patterns', 'and', 'assist', 'in', 'decisionmaking', 'they', 'aim', 'to', 'augment', 'human', 'intelligence', 'by', 'mimicking', 'the', 'way', 'the', 'human', 'brain', 'analyzes', 'and', 'responds', 'to', 'information']\n",
      "\n",
      "Filtered Words:\n",
      "['cognitive', 'computing', 'refers', 'advanced', 'systems', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'systems', 'learn', 'data', 'recognize', 'patterns', 'assist', 'decisionmaking', 'aim', 'augment', 'human', 'intelligence', 'mimicking', 'way', 'human', 'brain', 'analyzes', 'responds', 'information']\n",
      "Word Frequency Distribution (Excluding Stopwords):\n",
      "Counter({'human': 3, 'systems': 2, 'intelligence': 2, 'cognitive': 1, 'computing': 1, 'refers': 1, 'advanced': 1, 'simulate': 1, 'thought': 1, 'processes': 1, 'using': 1, 'artificial': 1, 'machine': 1, 'learning': 1, 'natural': 1, 'language': 1, 'processing': 1, 'learn': 1, 'data': 1, 'recognize': 1, 'patterns': 1, 'assist': 1, 'decisionmaking': 1, 'aim': 1, 'augment': 1, 'mimicking': 1, 'way': 1, 'brain': 1, 'analyzes': 1, 'responds': 1, 'information': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prithvi\n",
      "[nltk_data]     Ahuja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK resources \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence, \n",
    "machine learning, and natural language processing. These systems learn from data, recognize patterns, and assist in \n",
    "decision-making. They aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Convert text to lowercase and remove punctuation\n",
    "text_lower = text.lower()\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Cleaned Text:\")\n",
    "print(text_clean)\n",
    "\n",
    "# 2. Tokenize the text into words and sentence\n",
    "words = word_tokenize(text_clean)\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nTokens:\")\n",
    "print(words)\n",
    "\n",
    "# 3. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(\"\\nFiltered Words:\")\n",
    "print(filtered_words)\n",
    "\n",
    "# 4. Display word frequency distribution\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Word Frequency Distribution (Excluding Stopwords):\")\n",
    "print(word_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Stemming and Lemmatizaton\n",
    "1. Take the tokenized words from Quesion 1 (fer stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmatization using NLTK's WordNetLemmaizer.\n",
    "4. Compare and display results of both techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming (PorterStemmer): ['cognit', 'comput', 'refer', 'advanc', 'system', 'simul', 'human', 'thought', 'process', 'use', 'artifici', 'intellig', 'machin', 'learn', 'natur', 'languag', 'process', 'system', 'learn', 'data', 'recogn', 'pattern', 'assist', 'decisionmak', 'aim', 'augment', 'human', 'intellig', 'mimick', 'way', 'human', 'brain', 'analyz', 'respond', 'inform']\n",
      "\n",
      "Stemming (LancasterStemmer): ['cognit', 'comput', 'ref', 'adv', 'system', 'sim', 'hum', 'thought', 'process', 'us', 'art', 'intellig', 'machin', 'learn', 'nat', 'langu', 'process', 'system', 'learn', 'dat', 'recogn', 'pattern', 'assist', 'decisionmak', 'aim', 'aug', 'hum', 'intellig', 'mimick', 'way', 'hum', 'brain', 'analys', 'respond', 'inform']\n",
      "\n",
      "Lemmatization: ['cognitive', 'computing', 'refers', 'advanced', 'system', 'simulate', 'human', 'thought', 'process', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'system', 'learn', 'data', 'recognize', 'pattern', 'assist', 'decisionmaking', 'aim', 'augment', 'human', 'intelligence', 'mimicking', 'way', 'human', 'brain', 'analyzes', 'responds', 'information']\n"
     ]
    }
   ],
   "source": [
    "# Initialize stemmers and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming using PorterStemmer and LancasterStemmer\n",
    "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "# Apply lemmatization using WordNetLemmatizer\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "# Compare and display results of both techniques\n",
    "print(\"\\nStemming (PorterStemmer):\", porter_stemmed)\n",
    "print(\"\\nStemming (LancasterStemmer):\", lancaster_stemmed)\n",
    "print(\"\\nLemmatization:\", lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from QuesƟon 1.\n",
    "2. Use regular expressions to:\n",
    "a. Extract all words with more than 5 leƩers.\n",
    "b. Extract all numbers (if any exist in their text).\n",
    "c. Extract all capitalized words.\n",
    "3. Use text spliƫng techniques to:\n",
    "a. Split the text into words containing only alphabets (removing digits and special\n",
    "characters).\n",
    "b. Extract words starƟng with a vowel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with more than 5 letters: ['cognitive', 'computing', 'refers', 'advanced', 'systems', 'simulate', 'thought', 'processes', 'artificial', 'intelligence', 'machine', 'learning', 'natural', 'language', 'processing', 'systems', 'recognize', 'patterns', 'assist', 'decisionmaking', 'augment', 'intelligence', 'mimicking', 'analyzes', 'responds', 'information']\n",
      "\n",
      "Numbers found in text: []\n",
      "\n",
      "Capitalized words: []\n",
      "\n",
      "Alphabetic words: ['cognitive', 'computing', 'refers', 'to', 'advanced', 'systems', 'that', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'these', 'systems', 'learn', 'from', 'data', 'recognize', 'patterns', 'and', 'assist', 'in', 'decisionmaking', 'they', 'aim', 'to', 'augment', 'human', 'intelligence', 'by', 'mimicking', 'the', 'way', 'the', 'human', 'brain', 'analyzes', 'and', 'responds', 'to', 'information']\n",
      "\n",
      "Words starting with a vowel: ['advanced', 'using', 'artificial', 'intelligence', 'and', 'and', 'assist', 'in', 'aim', 'augment', 'intelligence', 'analyzes', 'and', 'information']\n"
     ]
    }
   ],
   "source": [
    "# 1. Use regular expressions to extract:\n",
    "# a. All words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', text_clean)\n",
    "print(\"\\nWords with more than 5 letters:\", long_words)\n",
    "\n",
    "# b. All numbers \n",
    "numbers = re.findall(r'\\b\\d+\\b', text_clean)\n",
    "print(\"\\nNumbers found in text:\", numbers)\n",
    "\n",
    "# c. All capitalized words\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text_clean)\n",
    "print(\"\\nCapitalized words:\", capitalized_words)\n",
    "\n",
    "# 2. Use text splitting techniques:\n",
    "# a. Split the text into words containing only alphabets \n",
    "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
    "print(\"\\nAlphabetic words:\", alphabetic_words)\n",
    "\n",
    "# b. Extract words starting with a vowel\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text_clean)\n",
    "print(\"\\nWords starting with a vowel:\", vowel_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "2. Write a custom tokenizaƟon funcƟon that:\n",
    "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
    "\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
    "a single token).\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
    "should remain as is).\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
    "'<PHONE>' placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Tokenized Text: ['cognitive', 'computing', 'refers', 'to', 'advanced', 'systems', 'that', 'simulate', 'human', 'thought', 'processes', 'using', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'natural', 'language', 'processing', 'these', 'systems', 'learn', 'from', 'data', 'recognize', 'patterns', 'and', 'assist', 'in', 'decision-making', 'they', 'aim', 'to', 'augment', 'human', 'intelligence', 'by', 'mimicking', 'the', 'way', 'the', 'human', 'brain', 'analyzes', 'and', 'responds', 'to', 'information']\n",
      "\n",
      "Text with Emails, URLs, and Phone Numbers Replaced:\n",
      "\n",
      "Cognitive computing refers to advanced systems that simulate human thought processes using artificial intelligence, \n",
      "machine learning, and natural language processing. These systems learn from data, recognize patterns, and assist in \n",
      "decision-making. They aim to augment human intelligence by mimicking the way the human brain analyzes and responds to information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Custom tokenization function\n",
    "def custom_tokenizer(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
    "\n",
    "# 2. Regex substitutions for cleaning\n",
    "# a. Replace email addresses with '<EMAIL>'\n",
    "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
    "# b. Replace URLs with '<URL>'\n",
    "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
    "# c. Replace phone numbers with '<PHONE>'\n",
    "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
    "\n",
    "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
    "print(text_cleaned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
